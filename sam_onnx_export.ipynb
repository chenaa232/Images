{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kYG5aPYlVOY",
        "outputId": "0c33bb8c-7f06-4ced-abe3-5c1c2fb940fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-y8fj5t6r\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-y8fj5t6r\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment_anything\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment_anything: filename=segment_anything-1.0-py3-none-any.whl size=36592 sha256=f0686950e5890fe973e78519eeb36b32e43eeeb7d66593376954099ec5ae1964\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-94e05lrx/wheels/15/d7/bd/05f5f23b7dcbe70cbc6783b06f12143b0cf1a5da5c7b52dcc5\n",
            "Successfully built segment_anything\n",
            "Installing collected packages: segment_anything\n",
            "Successfully installed segment_anything-1.0\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.17.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install onnx\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "source": [
        "from segment_anything import sam_model_registry\n",
        "from segment_anything.utils.onnx import SamOnnxModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "p82usQSalVOY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=b6bd922659f6152589f736a3e7c7625b4a71d8bd4b81c4c3651081a8b3b57c72\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "\n",
            "Saved under sam_vit_b_01ec64.pth\n"
          ]
        }
      ],
      "source": [
        "# Download SAM model checkpoint\n",
        "!pip install wget\n",
        "!python -m wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
      ],
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn4ho4R0lVOZ",
        "outputId": "2a0f6514-d436-42d7-caa4-b68d47cfdc7d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "# Load SAM model\n",
        "sam = sam_model_registry[\"vit_b\"](checkpoint=\"./sam_vit_b_01ec64.pth\")"
      ],
      "metadata": {
        "id": "iKa8tQrblVOZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/image_encoder.py:258: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if pad_h > 0 or pad_w > 0:\n",
            "/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/image_encoder.py:304: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
            "/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/image_encoder.py:304: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
            "/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/image_encoder.py:306: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if rel_pos.shape[0] != max_rel_dist:\n",
            "/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/image_encoder.py:318: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n",
            "/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/image_encoder.py:319: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n",
            "/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/image_encoder.py:320: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n",
            "/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/image_encoder.py:287: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if Hp > H or Wp > W:\n"
          ]
        }
      ],
      "source": [
        "# Export images encoder from SAM model to ONNX\n",
        "torch.onnx.export(\n",
        "    f=\"vit_b_encoder.onnx\",\n",
        "    model=sam.image_encoder,\n",
        "    args=torch.randn(1, 3, 1024, 1024),\n",
        "    input_names=[\"images\"],\n",
        "    output_names=[\"embeddings\"],\n",
        "    export_params=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdjbcHdmlVOZ",
        "outputId": "3a25fa3a-ece8-47d5-a4b6-c99211914eba"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/transformer.py:232: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  attn = attn / math.sqrt(c_per_head)\n",
            "/usr/local/lib/python3.11/dist-packages/segment_anything/utils/onnx.py:97: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  score_reweight = torch.tensor(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/onnx/symbolic_opset9.py:5383: UserWarning: Exporting aten::index operator of advanced indexing in opset 17 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Export mask decoder from SAM model to ONNX\n",
        "onnx_model = SamOnnxModel(sam, return_single_mask=True)\n",
        "embed_dim = sam.prompt_encoder.embed_dim\n",
        "embed_size = sam.prompt_encoder.image_embedding_size\n",
        "mask_input_size = [4 * x for x in embed_size]\n",
        "dummy_inputs = {\n",
        "    \"image_embeddings\": torch.randn(1, embed_dim, *embed_size, dtype=torch.float),\n",
        "    \"point_coords\": torch.randint(low=0, high=1024, size=(1, 5, 2), dtype=torch.float),\n",
        "    \"point_labels\": torch.randint(low=0, high=4, size=(1, 5), dtype=torch.float),\n",
        "    \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),\n",
        "    \"has_mask_input\": torch.tensor([1], dtype=torch.float),\n",
        "    \"orig_im_size\": torch.tensor([1500, 2250], dtype=torch.float),\n",
        "}\n",
        "output_names = [\"masks\", \"iou_predictions\", \"low_res_masks\"]\n",
        "torch.onnx.export(\n",
        "    f=\"vit_b_decoder.onnx\",\n",
        "    model=onnx_model,\n",
        "    args=tuple(dummy_inputs.values()),\n",
        "    input_names=list(dummy_inputs.keys()),\n",
        "    output_names=output_names,\n",
        "    dynamic_axes={\n",
        "        \"point_coords\": {1: \"num_points\"},\n",
        "        \"point_labels\": {1: \"num_points\"}\n",
        "    },\n",
        "    export_params=True,\n",
        "    opset_version=17,\n",
        "    do_constant_folding=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2uWDyDSlVOa",
        "outputId": "f9f56d6f-d78f-4571-8d2a-229abd1ee6d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "import sys\n",
        "!{sys.executable} -m pip install opencv-python matplotlib onnx onnxruntime\n",
        "\n",
        "\n",
        "!mkdir images\n",
        "!wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iD9xMgznH7Q",
        "outputId": "fd4d82b9-2a41-498b-a9a8-811d361e3c9b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "Torchvision version: 0.21.0+cu124\n",
            "CUDA is available: False\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.21.0\n",
            "--2025-04-18 06:36:24--  https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 271475 (265K) [image/jpeg]\n",
            "Saving to: ‘images/truck.jpg’\n",
            "\n",
            "truck.jpg           100%[===================>] 265.11K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-04-18 06:36:24 (6.79 MB/s) - ‘images/truck.jpg’ saved [271475/271475]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime\n",
        "from onnxruntime.quantization import QuantType\n",
        "from onnxruntime.quantization.quantize import quantize_dynamic"
      ],
      "metadata": {
        "id": "Bwo6VZcEm6Ih"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model_encoder_quantized_path = \"sam_encoder_onnx_quantized_example.onnx\"\n",
        "quantize_dynamic(\n",
        "    model_input='/content/vit_b_encoder.onnx',\n",
        "    model_output=onnx_model_encoder_quantized_path,\n",
        "    # optimize_model=True,\n",
        "    per_channel=False,\n",
        "    reduce_range=False,\n",
        "    weight_type=QuantType.QUInt8,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWFlCZSOnZHg",
        "outputId": "6ea2b134-ead9-446a-c4a8-0a6b88934093"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/blocks.0/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_param: \"unk__3\"\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/blocks.1/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_param: \"unk__75\"\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/blocks.3/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_param: \"unk__202\"\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/blocks.4/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_param: \"unk__284\"\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/blocks.6/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_param: \"unk__411\"\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/blocks.7/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_param: \"unk__493\"\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/blocks.9/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_param: \"unk__620\"\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/blocks.10/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_param: \"unk__702\"\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model_encoder_quantized_path = \"sam_decoder_onnx_quantized_example.onnx\"\n",
        "quantize_dynamic(\n",
        "    model_input='/content/vit_b_decoder.onnx',\n",
        "    model_output=onnx_model_encoder_quantized_path,\n",
        "    # optimize_model=True,\n",
        "    per_channel=False,\n",
        "    reduce_range=False,\n",
        "    weight_type=QuantType.QUInt8,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwCzgcZtnizA",
        "outputId": "d101f340-ae7f-413e-f50e-754a5a0f5cb2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6-RirRroHdo",
        "outputId": "575c91d1-4ac7-45e9-ca92-12ab01e33b35"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 816M\n",
            "drwxr-xr-x 2 root root 4.0K Apr 18 06:36 images\n",
            "-rw-r--r-- 1 root root 4.6M Apr 18 06:39 sam_decoder_onnx_quantized_example.onnx\n",
            "-rw-r--r-- 1 root root  96M Apr 18 06:38 sam_encoder_onnx_quantized_example.onnx\n",
            "drwxr-xr-x 1 root root 4.0K Apr 16 13:40 sample_data\n",
            "-rw-r--r-- 1 root root 358M Apr 18 06:31 sam_vit_b_01ec64.pth\n",
            "-rw-r--r-- 1 root root  16M Apr 18 06:33 vit_b_decoder.onnx\n",
            "-rw-r--r-- 1 root root 343M Apr 18 06:33 vit_b_encoder.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow\n",
        "!pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svaPvEBspIUg",
        "outputId": "03b7c346-d50d-4b49-96ee-746314b4185a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "fCAekF23pSgo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD IMAGE\n",
        "!wget https://github.com/AndreyGermanov/sam_onnx_full_export/blob/main/cat_dog.jpg\n",
        "img = Image.open(\"cat_dog.jpg\").convert(\"RGB\")\n",
        "img.size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVvY-ZPzpxvw",
        "outputId": "d3d6d6c2-15bc-4b9e-d5d3-98c3d6640cd2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-18 06:53:00--  https://github.com/AndreyGermanov/sam_onnx_full_export/blob/main/cat_dog.jpg\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘cat_dog.jpg.1’\n",
            "\n",
            "cat_dog.jpg.1           [ <=>                ] 201.22K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-04-18 06:53:00 (2.03 MB/s) - ‘cat_dog.jpg.1’ saved [206049]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(612, 415)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. PREPROCESS IMAGE FOR ENCODER\n",
        "\n",
        "# Resize image preserving aspect ratio using 1024 as a long side\n",
        "orig_width, orig_height = img.size\n",
        "resized_width, resized_height = img.size\n",
        "\n",
        "if orig_width > orig_height:\n",
        "    resized_width = 1024\n",
        "    resized_height = int(1024 / orig_width * orig_height)\n",
        "else:\n",
        "    resized_height = 1024\n",
        "    resized_width = int(1024 / orig_height * orig_width)\n",
        "\n",
        "img = img.resize((resized_width, resized_height), Image.Resampling.BILINEAR)\n",
        "\n",
        "img.size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv82UpZOp03I",
        "outputId": "657c270f-bb52-46a8-c5b9-c64f8fc9f7d1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1024, 694)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare input tensor from image\n",
        "input_tensor = np.array(img)\n",
        "\n",
        "# Normalize input tensor numbers\n",
        "mean = np.array([123.675, 116.28, 103.53])\n",
        "std = np.array([[58.395, 57.12, 57.375]])\n",
        "input_tensor = (input_tensor - mean) / std\n",
        "\n",
        "# Transpose input tensor to shape (Batch,Channels,Height,Width\n",
        "input_tensor = input_tensor.transpose(2,0,1)[None,:,:,:].astype(np.float32)\n",
        "\n",
        "input_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuhWHyPup21Y",
        "outputId": "270ffe74-55c1-492a-b410-4addd7402cec"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3, 694, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make image square 1024x1024 by padding short side by zeros\n",
        "if resized_height < resized_width:\n",
        "    input_tensor = np.pad(input_tensor,((0,0),(0,0),(0,1024-resized_height),(0,0)))\n",
        "else:\n",
        "    input_tensor = np.pad(input_tensor,((0,0),(0,0),(0,0),(0,1024-resized_width)))\n",
        "\n",
        "input_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5WakIr-p4c4",
        "outputId": "101cb408-02aa-4929-b4a1-0104d9b940c7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3, 1024, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. GET IMAGE EMBEDDINGS USING IMAGE ENCODER\n",
        "encoder = ort.InferenceSession(\"/content/sam_encoder_onnx_quantized_example.onnx\")\n",
        "outputs = encoder.run(None,{\"images\":input_tensor})\n",
        "embeddings = outputs[0]\n",
        "\n",
        "embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXlD2y02p59Y",
        "outputId": "161985bf-22c3-4ba1-ea0e-45e984818cd6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 256, 64, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ENCODE PROMPT (single point)\n",
        "input_point = np.array([[321,230]])\n",
        "input_label = np.array([1])\n",
        "\n",
        "onnx_coord = np.concatenate([input_point, np.array([[0.0, 0.0]])], axis=0)[None, :, :]\n",
        "onnx_label = np.concatenate([input_label, np.array([-1])])[None, :].astype(np.float32)\n",
        "\n",
        "coords = deepcopy(onnx_coord).astype(float)\n",
        "coords[..., 0] = coords[..., 0] * (resized_width / orig_width)\n",
        "coords[..., 1] = coords[..., 1] * (resized_height / orig_height)\n",
        "\n",
        "onnx_coord = coords.astype(\"float32\")\n",
        "onnx_coord"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h08q6FGgqQop",
        "outputId": "e683eb6e-ee43-4aac-8f44-88b49f706a7e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[537.098 , 384.6265],\n",
              "        [  0.    ,   0.    ]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN DECODER TO GET MASK\n",
        "onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n",
        "onnx_has_mask_input = np.zeros(1, dtype=np.float32)\n",
        "\n",
        "decoder = ort.InferenceSession(\"/content/sam_decoder_onnx_quantized_example.onnx\")\n",
        "masks,_,_ = decoder.run(None,{\n",
        "    \"image_embeddings\": embeddings,\n",
        "    \"point_coords\": onnx_coord,\n",
        "    \"point_labels\": onnx_label,\n",
        "    \"mask_input\": onnx_mask_input,\n",
        "    \"has_mask_input\": onnx_has_mask_input,\n",
        "    \"orig_im_size\": np.array([orig_height, orig_width], dtype=np.float32)\n",
        "})"
      ],
      "metadata": {
        "id": "fo99V_DsqUIY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POSTPROCESS MASK\n",
        "mask = masks[0][0]\n",
        "mask = (mask > 0).astype('uint8')*255\n",
        "mask.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArelmjfhqdSY",
        "outputId": "801fc4b3-ccfd-4730-e1f4-4e5c4ca19502"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(415, 612)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VISUALIZE MASK\n",
        "img_mask = Image.fromarray(mask,\"L\")\n",
        "img_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "63Q860ejqeVQ",
        "outputId": "d9a88370-9601-4ce2-dfc4-2b46bcf45316"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=612x415>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}